# BaristaBench: LLM Coffee Order Parser

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Kaggle](https://img.shields.io/badge/Kaggle-BaristaBench-green.svg)](https://www.kaggle.com/competitions/barista-bench)

## Table of Contents
- [Overview](#overview)
- [Author](#author)
- [Approach](#approach)
- [Key Features](#key-features)
- [Repository Structure](#repository-structure)
- [Data and Task](#data-and-task)
- [LLM Parsing and Pricing Logic](#llm-parsing-and-pricing-logic)
- [Results and Validation](#results-and-validation)
- [Setup and Installation](#setup-and-installation)
- [Usage](#usage)
- [Future Work](#future-work)
- [License](#license)

## Overview
This project is a solution to the Kaggle competition **[BaristaBench: The Reasoning Challenge](https://www.kaggle.com/competitions/barista-bench)**.

The goal is to turn **natural-language coffee orders** into a structured JSON format that a point-of-sale (POS) system can understand, while correctly applying a complex set of menu, sizing, modifier, and pricing rules.

Instead of training a conventional ML model, this solution uses **large language models (LLMs)** (OpenAI GPT-4.1 and Google Gemini) guided by a carefully engineered **system prompt plus few-shot examples**. The raw LLM output is then **post-processed**:

- to enforce the JSON schema,
- to merge duplicate items, and
- to recompute prices deterministically from a hand-coded menu.

The final predictions are written to a CSV submission file suitable for Kaggle.

## Author
- **Sashwat Desai** (`desai.sas@northeastern.edu`)
  - MS, Applied Mathematics
  - Northeastern University, Boston

## Approach
High-level approach for the BaristaBench challenge:

- Use a **system prompt** that encodes:
  - the full menu and prices,
  - size and modifier rules,
  - how to handle corrections ("actually", "cancel that", etc.), and
  - the exact JSON output schema.
- Add **few-shot examples** sampled from the training set directly into the system message to anchor model behavior.
- Call an LLM (OpenAI or Gemini) with the prompt and the order text to obtain a JSON-like string.
- **Validate and sanitize** the response:
  - Strip Markdown fences if present.
  - Parse as JSON and require `items` and `total_price` keys.
  - Merge identical items into single entries with higher quantity.
  - Recompute `total_price` from a deterministic menu pricing function, ignoring the model's arithmetic.
- Run this pipeline for all test orders, then save a **Kaggle submission CSV**.

## Key Features
- **LLM-based parser** for complex natural-language coffee orders.
- **Rich system prompt** encoding menu, sizes, modifiers, and correction logic.
- **Few-shot conditioning** using real labeled training examples from Kaggle.
- **Deterministic pricing engine** that:
  - recomputes prices from static Python dictionaries,
  - supports drink sizes and stacked modifiers,
  - cleanly separates food from drinks.
- **Robust JSON parsing** with retries and rate-limit handling.
- **Batch inference pipeline** with progress bars and configurable delays.
- **Quality checks** against training data:
  - price validator for the entire training set,
  - quick accuracy sample,
  - detailed structural error breakdown (names, sizes, quantities, modifiers).

## Repository Structure
After organization, the repository is structured as:

- **`LICENSE`** – MIT license for the project.
- **`requirements.txt`** – Python dependencies for running the notebooks.
- **`notebooks/`** – Jupyter notebooks.
  - **`barista_bench.ipynb`** – original exploratory notebook.
  - **`barista_bench_updated.ipynb`** – main, commented solution notebook and pipeline.
- **`data/`** – Kaggle competition data (not version-controlled on Kaggle itself).
  - **`train.csv`** – training orders with labeled JSON.
  - **`test.csv`** – test orders to predict on.
  - **`sample_submission.csv`** – example submission template from Kaggle.
- **`submissions/`** – model outputs.
  - **`submission.csv`** – latest submission generated by the notebook.
  - **`submission_openai_gpt-4.1.csv`** – submission generated with OpenAI GPT-4.1.
- **`docs/`** – documentation and supporting materials.
  - **`barista_bench_explanation.txt`** – detailed, cell-by-cell explanation of the main notebook.
  - **`menu.md`** – textual description of the menu and rules.
- **`assets/`** – images or score screenshots.
  - **`Updated Score - 02:20.png`** – screenshot of a leaderboard/score update.

## Data and Task
The Kaggle competition provides:

- **Training data (`train.csv`)** with columns including:
  - `order`: natural-language description of a coffee shop order.
  - `expected_json`: ground-truth JSON satisfying the competition schema.
- **Test data (`test.csv`)** with columns including:
  - `id`: unique identifier for each order.
  - `order`: natural-language order text without labels.

The task is to produce a **`predicted_json`** column for the test set that:

- adheres strictly to the specified JSON schema,
- correctly tracks corrections and cancellations in the order text,
- and applies the menu’s pricing rules exactly.

## LLM Parsing and Pricing Logic
The core logic lives in `barista_bench_updated.ipynb` and follows these stages:

1. **Menu and pricing rules**
   - Define Python dictionaries for base prices, size adjustments, and modifier costs.
   - Mark which items are food (no size, no drink modifiers).

2. **Price validator (`fix_price`)**
   - Given a parsed JSON structure, recompute `total_price` as:
     \[(base\_price + size\_adj + modifier\_sum) \times quantity\]
   - This is used both to verify the training labels and to override LLM price outputs.

3. **Duplicate merging (`merge_items`)**
   - Collapses items with the same `(name, size, modifiers)` into a single line with higher `quantity`.

4. **System prompt and examples**
   - A long system message encodes:
     - the menu, sizes, modifiers, and pricing rules,
     - rules for handling phrases like "actually", "scratch that", and cancellations,
     - and the exact JSON schema.
   - Few-shot examples from `train.csv` are appended to the prompt to guide the model.

5. **LLM client and `call_llm` wrapper**
   - Supports two providers:
     - **OpenAI**: via the `OpenAI` client and Chat Completions.
     - **Gemini**: via the `google.genai` client.
   - Abstracts provider-specific API calls behind a single function that always returns text.

6. **`parse_order` function**
   - Calls the LLM with the prompt and an order.
   - Strips Markdown fences and parses the output as JSON.
   - Checks for required keys and applies `merge_items` and `fix_price`.
   - Retries on JSON/validation errors and handles rate-limit errors with backoff.
   - Returns a safe fallback (`{"items": [], "total_price": 0.0}`) if all retries fail.

7. **Batch prediction loop**
   - Iterates over all rows in `test.csv`.
   - Calls `parse_order` for each `order` with a configurable delay between calls.
   - Stores predictions as JSON strings in a list, then writes them to CSV.

## Results and Validation
The notebook includes several sanity checks:

- **Price validator check (training set)**
  - Runs `fix_price` over all labeled `expected_json` in `train.csv` and reports the fraction
    of rows where the recomputed `total_price` exactly matches the label.

- **Quick accuracy sample (training set)**
  - Samples 20 training orders, re-parses them through the full pipeline, and checks whether:
    - the set of item names matches, and
    - prices are within 1 cent of the labels.

- **Deeper structural validation**
  - For a sample of 50 training orders, compares expected vs predicted on:
    - item counts,
    - names,
    - sizes,
    - quantities,
    - modifiers.
  - Logs a small number of example mismatches per category to guide prompt tuning.

These provide a mixture of exact numeric validation and structural diagnostics.

## Setup and Installation

### Prerequisites
- Python 3.11+
- An OpenAI API key and/or a Google Gemini API key
- A Kaggle download of the **BaristaBench** dataset placed into the `data/` folder if not
  already present

### Installation
```bash
# Clone this repository
git clone https://github.com/desai-sashwat/barista-bench.git
cd barista-bench

# (Optional but recommended) create a virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Environment Variables
Set your API keys as environment variables before running the notebooks:

```bash
export GEMINI_KEY="your-gemini-api-key"
export OPENAI_KEY="your-openai-api-key"
```

On Windows PowerShell:

```powershell
$env:GEMINI_KEY="your-gemini-api-key"
$env:OPENAI_KEY="your-openai-api-key"
```

## Usage

### Running the main notebook
1. Start Jupyter Lab or Jupyter Notebook:
   ```bash
   jupyter lab
   ```

2. Open `notebooks/barista_bench_updated.ipynb`.

3. Make sure the kernel uses the Python environment where you installed the dependencies.

4. Run the cells sequentially:
   - Data loading (`train.csv`, `test.csv`).
   - Menu and pricing logic.
   - System prompt and examples.
   - Model configuration (`PROVIDER`, `MODEL`).
   - Batch parsing over the test set.
   - Submission file creation.
   - Validation and offline checks.

5. The final Kaggle submission file(s) will be written to `submissions/`.

### Generating a new submission
- To switch models, edit the configuration cell:
  ```python
  PROVIDER = "openai"  # or "gemini"
  MODEL = "gpt-4.1"    # or your chosen Gemini model
  ```
- Re-run the parsing and submission cells.
- Upload the new CSV from `submissions/` to the Kaggle **BaristaBench** competition page.

## Future Work
- Experiment with additional **prompt variants** and few-shot strategies.
- Use **structured output APIs** (JSON schemas) where available to reduce parsing failures.
- Introduce a small **post-hoc rule-based layer** to handle known edge cases and ambiguous
  phrases more deterministically.
- Add support for **multiple provider fallbacks** (e.g., try Gemini first, then OpenAI).
- Refine **error analysis notebooks** to visualize common failure modes.

## License
This project is licensed under the MIT License – see the [LICENSE](LICENSE) file for details.
